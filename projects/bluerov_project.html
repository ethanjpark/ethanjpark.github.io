<!DOCTYPE html>
<html>
  <head>
    <title>Ethan Park</title>
    <link href='https://fonts.googleapis.com/css?family=Roboto Slab' rel='stylesheet'>
    <link rel="stylesheet" href="../css/bluerov_project.css">
    <link rel="icon" type="image/png" href="../links/index/favicon.png">
  </head>
  <body>
    <button onclick="window.location.href = 'https://ethanjpark.github.io';" class="home">Home</button>
    <div class="header">
      Feedback Control of BlueROV2<br>
      <div class="quarter">
        Spring/Fall 2019
      </div>
    </div>

    <div class="first">
      This is my MSR final project, advised by Dr. Mitra Hartmann, Hannah Emnett, and Kevin Kleczka. The project was driven by a need for an underwater robotic platform for the ongoing research conducted by Dr. Hartmann and her group. The ultimate goal was to have an underwater robot autonomously controlled by sensory feedback.
      <br><br>
      Since her group did not have a suitable robot, the first task was to explore commercially available underwater robots and purchase one. One option was educational robot kits from SeaMATE, who offered four kits targeted at different skill/grade levels, ranging from elementary to high school. In a nutshell, these kits would result in a space-frame robot made out of PVC pipes and run by an Arduino. The second option was one of the ubiquitous remote control underwater drones, targeted at boaters, fishermen, and divers. 
      <br><br>
      Both of these options came with significant downsides. The kits required a significant time investment for construction and an Arduino was not computationally powerful enough; replacing it meant significant changes to the provided electronics, which meant even more time and effort just to have a satisfactory working robot. The commercial drones were great in that they worked out of the box, however, it was obvious that none were meant to be tampered with. At the very least, opening one up to access the internal electronics could compromise the internal seal from outside water. In addition, most of these drones were controlled via a proprietary controller or mobile app, neither with any sort of API. Therefore, trying to hijack control of the drone from the user end would not be a trivial task, either.
    </div>
    <div class="second">
      <img src="../links/bluerov/bluerov2.png" class="bluerov2_pic"/>
      <div class="text1">
        Ultimately, we settled on the <a href="https://bluerobotics.com/store/rov/bluerov2/" style="color: #00ccff" target="_blank">BlueRobotics BlueROV2</a> shown on the left, a nice compromise between a kit and a finished, commercial product. Although the robot came unassembled, all of the individual components were already made and tested, meaning that the user only had to put the pieces together, which also only took about seven hours - compared to the double digits required for the aforementioned kits. Much of the hardware, electronics, and software was open source, with anything proprietary being heavily documented online. In addition, all of this was easily accessible and somewhat receptive to further modification (BlueRobotics sells additional sensors and accessories, obviously with instructions on how to integrate them, but anything custom is up to the user.)
      </div>
      <div class="text2">
        As previously mentioned, the software and electronics on the BlueROV2 is open source. Specifically, the controller software is ArduSub, a derivative of the widespread ArduPilot software used on aerial drones, run on a Pixhawk autopilot board. The Pixhawk is also connected to a Raspberry Pi, which acts as a companion computer, a device that receives vehicle data from the autopilot for further use, usually on some sort of ground control station (GCS) software. This data is then transmitted through the robot's tether to a topside computer, which then displays this data via QGroundControl, a GCS software. The software stack and a generalized hardware flowchart are shown below. 
      </div>
      <img src="../links/bluerov/bluerov_software_stack.png" class="stack"/>
      <img src="../links/bluerov/rov_hardware.png" class="hardware"/>
    </div>

    <div class="third">
      <div class="text3">
         
      </div>
      <div class="text4">
        
      </div>
      <img src="../links/uuvsim/bbp.png" class="bbp"/>
      <figcaption class="bbpcaption">
        J. A. Farrell, S. Pang, W. Li, "Chemical plume tracing via an autonomous underwater vehicle", IEEE Journal Of Oceanic Engineering, vol. 30, no. 2, pp. 428-442, April 2005.
      </figcaption>
      <div class="text5">
        To briefly summarize the planning, the AUV will traverse the operational area until it finds the plume. The robot then zigzags up the plume, backtracking as necessary when it loses contact with the plume, until it can determine a source based on a stored list of contact positions. The demo video of the plume tracing in action is embedded below. Since the simulation is not very fast, the video has been sped up. The white spherical marker that appears near the robot at the end of the demo is where the robot thinks the plume source is.
      </div>
    </div>
    <div class="video">
        <iframe width="960" height="540"
        src="https://youtube.com/embed/hFxBnqc86v0"
        style="border:3px solid #00ccff;">
        </iframe>
    </div>
    <div class="text6">
      Please refer to the <a href="https://github.com/ethanjpark/Plume_Tracing_UUV_Sim" style="color: #00ccff" target="_blank">project repository</a> for details.
    </div>
  </body>
</html>